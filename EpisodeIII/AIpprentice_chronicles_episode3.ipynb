{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    " \n",
    "In this episode we will again use transfer learning with a transformer model. Transformers have revolutionized the field of natural language processing (NLP) and have found their way into various applications, including object detection.\n",
    "                \n",
    "Following the object detection tutorial at huggingface, we use the Detection Transformer (DETR) model. DETR is a state-of-the-art object detection model that combines the power of transformers with the task of object detection It consists of two main components: a CNN backbone and a transformer-based detection head. The backbone CNN extracts image features, which are then passed to the transformer-based detection head.\n",
    "              \n",
    "As in the previous section, most of the work here consists in prepraring the dataset to fit the model requirements and then follow the huggingface transformer [tutorial](https://huggingface.co/docs/transformers/tasks/object_detection) to complete the process. Specifically, we use the pre-trained model facebook/detr-resnet-50 available at huggignface hub. The model was train on the COCO 2017 object detection dataset and we need to process our data to follow the COCO format to fine-tune the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Using the scripts implemented in the first episode we generate a dataset containing 1000 labelled images and display the first 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "# Define a dictionary to store information about different objects\n",
    "objects = {\n",
    "    0: {'name': 'Sun', 'file': 'sun.png', 'code': 'SU'},\n",
    "    1: {'name': 'Earth', 'file': 'earth.png', 'code': 'EA'},\n",
    "    2: {'name': 'Mars', 'file': 'mars.png', 'code': 'MA'},\n",
    "    3: {'name': 'Venus', 'file': 'venus.png', 'code': 'VE'},\n",
    "    4: {'name': 'Jupiter', 'file': 'jupiter.png', 'code': 'JU'},\n",
    "    5: {'name': 'Mercury', 'file': 'mercury.png', 'code': 'ME'},\n",
    "    6: {'name': 'Saturn', 'file': 'saturn2.png', 'code': 'SA'},\n",
    "    7: {'name': 'Neptune', 'file': 'neptune.png', 'code': 'NE'},\n",
    "    8: {'name': 'Uranus', 'file': 'uranus.png', 'code': 'UR'},\n",
    "    9: {'name': 'Asteroid', 'file': 'asteroid.png', 'code': 'AS'},\n",
    "    10: {'name': 'Black Hole', 'file': 'black-hole.png', 'code': 'BL'},\n",
    "    11: {'name': 'Star Destroyer', 'file': 'star-destroyer.png', 'code': 'ST'}\n",
    "}\n",
    "\n",
    "# Get the total number of objects\n",
    "n_objects = len(objects)\n",
    "\n",
    "# Define a list to store background images\n",
    "bg = []\n",
    "bg.append(Image.open(r\"../img/background/bg1.jpg\"))\n",
    "bg.append(Image.open(r\"../img/background/bg2.jpg\"))\n",
    "bg.append(Image.open(r\"../img/background/bg3.jpg\"))\n",
    "\n",
    "# Set the directory path for object images\n",
    "im_dir = '../img/objects'\n",
    "\n",
    "# Loop through each object in the objects dictionary\n",
    "for class_id, values in objects.items():\n",
    "    # Open and convert the image file for the current planet\n",
    "    png_file = Image.open(os.path.join(im_dir, values['file'])).convert('RGBA')\n",
    "    \n",
    "    # Crop the image to a square shape using the maximum dimension\n",
    "    png_file = png_file.crop((0, 0, np.max(png_file.size), np.max(png_file.size)))\n",
    "    \n",
    "    # Store the processed image in the objects dictionary\n",
    "    objects[class_id]['image'] = png_file\n",
    "\n",
    "\n",
    "# Function to get a random background image and crop it\n",
    "def get_bg(bg): \n",
    "    id_bg = np.random.randint(0, 3)  # Randomly select a background image index\n",
    "    bg_tmp = bg[id_bg]  # Get the selected background image\n",
    "    (w, h) = bg_tmp.size  # Get the dimensions of the background image\n",
    "    x1 = np.random.randint(0, w - bg_size)  # Randomly choose x-coordinate for cropping\n",
    "    y1 = np.random.randint(0, h - bg_size)  # Randomly choose y-coordinate for cropping\n",
    "    bg_tmp = bg_tmp.crop((x1, y1, x1 + bg_size, y1 + bg_size))  # Crop the background image\n",
    "    return bg_tmp\n",
    "\n",
    "# Function to overlay a planet onto the background image\n",
    "def put_object(obj, bg_tmp):\n",
    "    h = obj.size[0]  # Get the size of the object image\n",
    "    if h >= bg_size:\n",
    "        scale = 0.4 * np.random.random() + 0.1  # Randomly choose a scale for large object\n",
    "    else:\n",
    "        scale = 0.7 * np.random.random() + 0.1  # Randomly choose a scale for small object\n",
    "    h = np.int32(scale * h)  # Calculate the new size of the object image based on the scale\n",
    "    p = obj.resize((h, h))  # Resize the object image\n",
    "    h_bg = bg_tmp.size[0]  # Get the size of the background image\n",
    "    x = np.random.randint(0, h_bg - h)  # Randomly choose x-coordinate for placing the object\n",
    "    y = np.random.randint(0, h_bg - h)  # Randomly choose y-coordinate for placing the object\n",
    "    bg_tmp.paste(p, (x, y), mask=p)  # Paste the object onto the background image\n",
    "    return bg_tmp, x, y, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_img = 1000   # number of images  \n",
    "bg_size = 800  # Size of the background image\n",
    "im_size = 256  # Size of the resized images\n",
    "\n",
    "# Function to create a single example for the dataset\n",
    "def create_example():\n",
    "    class_id = np.random.randint(0, n_objects)  # Randomly choose a class ID\n",
    "    bg_tmp = get_bg(bg)  # Get a random background image\n",
    "    plan_im = objects[class_id]['image']  # Get the image of the chosen class\n",
    "    img, x, y, h = put_object(plan_im, bg_tmp)  # Overlay the object on the background\n",
    "    img = img.resize((im_size, im_size))  # Resize the image to the desired size\n",
    "    x1 = np.float32(x) / bg_size  # Normalize x-coordinate of the object's position\n",
    "    y1 = np.float32(y) / bg_size  # Normalize y-coordinate of the object's position\n",
    "    h = np.float32(h) / bg_size  # Normalize size of the object\n",
    "    return img, class_id, x1, y1, h\n",
    "\n",
    "# Function to create the dataset\n",
    "def create_dataset(set_size):\n",
    "    dataset = []\n",
    "    for i in tqdm(range(set_size)):\n",
    "        image, class_id, x1, y1, h = create_example()\n",
    "        dataset.append([image, class_id, x1, y1, h])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "print('Generating training set...')\n",
    "dataset = create_dataset(n_img)  # Create a dataset of 1000 examples\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set is now processed to be loaded as a huggingface DataSet instace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newset=[]\n",
    "\n",
    "for i,data in enumerate(dataset):\n",
    "     width = data[0].size[0]\n",
    "     height = data[0].size[1]\n",
    "     newdata={}\n",
    "     newdata[\"image_id\"] = i\n",
    "     newdata[\"image\"] = data[0]\n",
    "     newdata[\"width\"] = width\n",
    "     newdata[\"height\"] = height\n",
    "     newdata[\"objects\"] = {\n",
    "         \"id\":[1],\n",
    "         \"area\": [0],\n",
    "         \"bbox\": [[data[2]*width,data[3]*height,data[4]*width,data[4]*height]],\n",
    "         \"category\": [data[1]]\n",
    "     }\n",
    "     newset.append(newdata)\n",
    "     \n",
    "from datasets import Dataset\n",
    "ds = Dataset.from_list(newset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a transformation pipeline to transform the dataset into the COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"facebook/detr-resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "import albumentations\n",
    "import torch\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "    #    albumentations.HorizontalFlip(p=1.0),\n",
    "    #    albumentations.RandomBrightnessContrast(p=1.0),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\",label_fields=[\"category\"]),\n",
    ")\n",
    "\n",
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# transforming a batch\n",
    "def transform_aug_ann(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")\n",
    "\n",
    "ds_new = ds.with_transform(transform_aug_ann)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "categories = [objects[id_][\"name\"] for id_ in objects]\n",
    "id2label = {index: x for index, x in enumerate(categories, start =0)}\n",
    "label2id = {v: k for k,v in id2label.items()}\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"deep-wars\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=30,\n",
    "    fp16=True,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=ds_new,\n",
    "    tokenizer=image_processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image, class_id, x1, y1, h = create_example()\n",
    "image_1= image.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"deep-wars/checkpoint-3600\") # adapt to your setting\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"deep-wars/checkpoint-3600\")  # adapt to your setting\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the object is not detected adjust the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image_1)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x, y2), model.config.id2label[label.item()], fill=\"white\")\n",
    "\n",
    "image_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x, y2), model.config.id2label[label.item()], fill=\"white\")\n",
    "\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
